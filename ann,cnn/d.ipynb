{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN에서 필요한 라이브러리 IMPORT\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "train_dir = \"./MNIST/cats_and_dogs/train\"\n",
    "test_dir = \"./MNIST/cats_and_dogs/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageDataGenerator 생성\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator 설정\n",
    "train_genrator = train_datagen.flow_from_directory(\n",
    "    train_dir, # 학습용 이미지를 가져올 폴더 명\n",
    "    classes=[\"cats\", \"dogs\"],     # cats 폴더의 image는 label을 0으로\n",
    "                                 # dogs 폴더의 image는 label을 1으로 설정\n",
    "    target_size=(150,150),       # 이미지 (150, 150)를 resize\n",
    "    batch_size=20,               # 한번에 20개의 이미지만 사용\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "# ImageDataGenerator 설정\n",
    "test_genrator = train_datagen.flow_from_directory(\n",
    "    test_dir, # 평가용 이미지를 가져올 폴더 명\n",
    "    classes=[\"cats\", \"dogs\"],     # cats 폴더의 image는 label을 0으로\n",
    "                                 # dogs 폴더의 image는 label을 1으로 설정\n",
    "    target_size=(150,150),       # 이미지 (150, 150)를 resize\n",
    "    batch_size=20,               # 한번에 20개의 이미지만 사용\n",
    "    class_mode=\"binary\"          # 이진분류인 경우 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model 구현\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 32x32; Received: input_shape=(15, 150, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m \u001b[39mimport\u001b[39;00m VGG16\n\u001b[1;32m----> 3\u001b[0m model_base \u001b[39m=\u001b[39m VGG16(weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m                     include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m                     input_shape\u001b[39m=\u001b[39;49m(\u001b[39m15\u001b[39;49m,\u001b[39m150\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39madd(model_base)\n\u001b[0;32m      9\u001b[0m model\u001b[39m.\u001b[39madd(Flatten())\n",
      "File \u001b[1;32mc:\\Users\\KOREAIT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\applications\\vgg16.py:137\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    133\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas true, `classes` should be 1000.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived `classes=\u001b[39m\u001b[39m{\u001b[39;00mclasses\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[0;32m    138\u001b[0m     input_shape,\n\u001b[0;32m    139\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m,\n\u001b[0;32m    140\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m    141\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[0;32m    142\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[0;32m    143\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m    144\u001b[0m )\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[1;32mc:\\Users\\KOREAIT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:408\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m                 )\n\u001b[0;32m    405\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    406\u001b[0m                 input_shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m min_size\n\u001b[0;32m    407\u001b[0m             ) \u001b[39mor\u001b[39;00m (input_shape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_size):\n\u001b[1;32m--> 408\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mInput size must be at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39m; Received: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m                 )\n\u001b[0;32m    413\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m require_flatten:\n",
      "\u001b[1;31mValueError\u001b[0m: Input size must be at least 32x32; Received: input_shape=(15, 150, 3)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model_base = VGG16(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(15,150,3))\n",
    "\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=256,\n",
    "                activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1,\n",
    "                activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_genrator,\n",
    "                steps_per_epoch=241,\n",
    "                epochs=30,\n",
    "                verbose=1,\n",
    "                validation_data=test_genrator,\n",
    "                validation_steps=241)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e191b7f8a7ade697261a80d73a4145313c00cd250ab8fbf5731ef43dc12a84bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
